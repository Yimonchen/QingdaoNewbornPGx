{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4d3c5f",
   "metadata": {},
   "source": [
    "## 使用xgboost训练功能预测分类模型\n",
    "\n",
    "使用xgboost提升树模型训练分类算法，基于树模型的特征分裂特性，便于在ensemble leanrning中筛选重要程度高的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ee854",
   "metadata": {},
   "source": [
    "## xgboost是如何处理缺失值的呢？\n",
    "\n",
    "在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销。\n",
    "\n",
    "在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。\n",
    "\n",
    "如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2274c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from sklearn.metrics import (f1_score, precision_score, recall_score, \n",
    "balanced_accuracy_score, roc_auc_score, auc, precision_recall_curve)\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a9709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"D:/functional-prediction/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddcfd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variant = pd.read_csv(\"output/variant_feat_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa22a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df_variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb4b41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed1 = 43987334\n",
    "seed2 = 39623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4dc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold split\n",
    "# good seeds: (43987334, 39623)\n",
    "# 划分数据集\n",
    "function_dict = {}\n",
    "fold = 6 # 1 test set + 5 train-validation set \n",
    "\n",
    "for function, content in df_variant.groupby([\"function\"]):\n",
    "    content = content.sample(frac=1, random_state=seed1).reset_index(drop=True)\n",
    "\n",
    "    fold_size = int(len(content) / fold) + 1\n",
    "\n",
    "    content_list = []\n",
    "    for i in range(fold):\n",
    "        content_part = content.iloc[i * fold_size: (i + 1) * fold_size]\n",
    "        content_part.index = range(len(content_part))\n",
    "        content_list.append(content_part)\n",
    "    \n",
    "    function_dict[function] = content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d4d7df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.78486\n",
      "[1]\tvalidation_0-auc:0.81285\n",
      "[2]\tvalidation_0-auc:0.81533\n",
      "[3]\tvalidation_0-auc:0.82127\n",
      "[4]\tvalidation_0-auc:0.81444\n",
      "[5]\tvalidation_0-auc:0.82488\n",
      "[6]\tvalidation_0-auc:0.82181\n",
      "[7]\tvalidation_0-auc:0.82590\n",
      "[8]\tvalidation_0-auc:0.82991\n",
      "[9]\tvalidation_0-auc:0.83200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\drkg\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalidation_0-auc:0.82421\n",
      "[11]\tvalidation_0-auc:0.82209\n",
      "[12]\tvalidation_0-auc:0.82718\n",
      "[13]\tvalidation_0-auc:0.82859\n",
      "[14]\tvalidation_0-auc:0.82786\n",
      "[15]\tvalidation_0-auc:0.82905\n",
      "[16]\tvalidation_0-auc:0.83045\n",
      "[17]\tvalidation_0-auc:0.83486\n",
      "[18]\tvalidation_0-auc:0.83533\n",
      "[19]\tvalidation_0-auc:0.83405\n",
      "[20]\tvalidation_0-auc:0.83191\n",
      "[21]\tvalidation_0-auc:0.83526\n",
      "[22]\tvalidation_0-auc:0.83062\n",
      "[23]\tvalidation_0-auc:0.83074\n",
      "[24]\tvalidation_0-auc:0.82994\n",
      "[25]\tvalidation_0-auc:0.82835\n",
      "[26]\tvalidation_0-auc:0.82786\n",
      "[27]\tvalidation_0-auc:0.82994\n",
      "[28]\tvalidation_0-auc:0.83210\n",
      "[29]\tvalidation_0-auc:0.83118\n",
      "[30]\tvalidation_0-auc:0.83258\n",
      "[31]\tvalidation_0-auc:0.82903\n",
      "[32]\tvalidation_0-auc:0.82715\n",
      "[33]\tvalidation_0-auc:0.82756\n",
      "[34]\tvalidation_0-auc:0.82713\n",
      "[35]\tvalidation_0-auc:0.82792\n",
      "[36]\tvalidation_0-auc:0.82646\n",
      "[37]\tvalidation_0-auc:0.82882\n",
      "[38]\tvalidation_0-auc:0.82784\n",
      "[39]\tvalidation_0-auc:0.82717\n",
      "[40]\tvalidation_0-auc:0.82826\n",
      "[41]\tvalidation_0-auc:0.83170\n",
      "[42]\tvalidation_0-auc:0.83329\n",
      "[43]\tvalidation_0-auc:0.83396\n",
      "[44]\tvalidation_0-auc:0.83298\n",
      "[45]\tvalidation_0-auc:0.83390\n",
      "[46]\tvalidation_0-auc:0.83181\n",
      "[47]\tvalidation_0-auc:0.83457\n",
      "[48]\tvalidation_0-auc:0.83524\n",
      "[0]\tvalidation_0-auc:0.81365\n",
      "[1]\tvalidation_0-auc:0.86673\n",
      "[2]\tvalidation_0-auc:0.86819\n",
      "[3]\tvalidation_0-auc:0.86586\n",
      "[4]\tvalidation_0-auc:0.85906\n",
      "[5]\tvalidation_0-auc:0.86786\n",
      "[6]\tvalidation_0-auc:0.86840\n",
      "[7]\tvalidation_0-auc:0.86812\n",
      "[8]\tvalidation_0-auc:0.87283\n",
      "[9]\tvalidation_0-auc:0.87036\n",
      "[10]\tvalidation_0-auc:0.86901\n",
      "[11]\tvalidation_0-auc:0.87030\n",
      "[12]\tvalidation_0-auc:0.87372\n",
      "[13]\tvalidation_0-auc:0.87134\n",
      "[14]\tvalidation_0-auc:0.87414\n",
      "[15]\tvalidation_0-auc:0.87777\n",
      "[16]\tvalidation_0-auc:0.87501\n",
      "[17]\tvalidation_0-auc:0.87332\n",
      "[18]\tvalidation_0-auc:0.87504\n",
      "[19]\tvalidation_0-auc:0.87907\n",
      "[20]\tvalidation_0-auc:0.88064\n",
      "[21]\tvalidation_0-auc:0.87954\n",
      "[22]\tvalidation_0-auc:0.88053\n",
      "[23]\tvalidation_0-auc:0.88279\n",
      "[24]\tvalidation_0-auc:0.88060\n",
      "[25]\tvalidation_0-auc:0.87999\n",
      "[26]\tvalidation_0-auc:0.88188\n",
      "[27]\tvalidation_0-auc:0.88303\n",
      "[28]\tvalidation_0-auc:0.88574\n",
      "[29]\tvalidation_0-auc:0.88561\n",
      "[30]\tvalidation_0-auc:0.88671\n",
      "[31]\tvalidation_0-auc:0.88523\n",
      "[32]\tvalidation_0-auc:0.88383\n",
      "[33]\tvalidation_0-auc:0.88257\n",
      "[34]\tvalidation_0-auc:0.88338\n",
      "[35]\tvalidation_0-auc:0.88416\n",
      "[36]\tvalidation_0-auc:0.88386\n",
      "[37]\tvalidation_0-auc:0.88386\n",
      "[38]\tvalidation_0-auc:0.88227\n",
      "[39]\tvalidation_0-auc:0.88257\n",
      "[40]\tvalidation_0-auc:0.88255\n",
      "[41]\tvalidation_0-auc:0.88275\n",
      "[42]\tvalidation_0-auc:0.88085\n",
      "[43]\tvalidation_0-auc:0.88152\n",
      "[44]\tvalidation_0-auc:0.88134\n",
      "[45]\tvalidation_0-auc:0.88194\n",
      "[46]\tvalidation_0-auc:0.88292\n",
      "[47]\tvalidation_0-auc:0.88292\n",
      "[48]\tvalidation_0-auc:0.88414\n",
      "[49]\tvalidation_0-auc:0.88494\n",
      "[50]\tvalidation_0-auc:0.88464\n",
      "[51]\tvalidation_0-auc:0.88494\n",
      "[52]\tvalidation_0-auc:0.88555\n",
      "[53]\tvalidation_0-auc:0.88447\n",
      "[54]\tvalidation_0-auc:0.88484\n",
      "[55]\tvalidation_0-auc:0.88408\n",
      "[56]\tvalidation_0-auc:0.88408\n",
      "[57]\tvalidation_0-auc:0.88409\n",
      "[58]\tvalidation_0-auc:0.88695\n",
      "[59]\tvalidation_0-auc:0.88597\n",
      "[60]\tvalidation_0-auc:0.88719\n",
      "[61]\tvalidation_0-auc:0.88716\n",
      "[62]\tvalidation_0-auc:0.88894\n",
      "[63]\tvalidation_0-auc:0.88887\n",
      "[64]\tvalidation_0-auc:0.88936\n",
      "[65]\tvalidation_0-auc:0.88808\n",
      "[66]\tvalidation_0-auc:0.88858\n",
      "[67]\tvalidation_0-auc:0.88778\n",
      "[68]\tvalidation_0-auc:0.88974\n",
      "[69]\tvalidation_0-auc:0.88986\n",
      "[70]\tvalidation_0-auc:0.88958\n",
      "[71]\tvalidation_0-auc:0.88940\n",
      "[72]\tvalidation_0-auc:0.88970\n",
      "[73]\tvalidation_0-auc:0.89047\n",
      "[74]\tvalidation_0-auc:0.89047\n",
      "[75]\tvalidation_0-auc:0.89127\n",
      "[76]\tvalidation_0-auc:0.89069\n",
      "[77]\tvalidation_0-auc:0.89069\n",
      "[78]\tvalidation_0-auc:0.89167\n",
      "[79]\tvalidation_0-auc:0.89195\n",
      "[80]\tvalidation_0-auc:0.89195\n",
      "[81]\tvalidation_0-auc:0.89018\n",
      "[82]\tvalidation_0-auc:0.88938\n",
      "[83]\tvalidation_0-auc:0.89018\n",
      "[84]\tvalidation_0-auc:0.88941\n",
      "[85]\tvalidation_0-auc:0.89000\n",
      "[86]\tvalidation_0-auc:0.89048\n",
      "[87]\tvalidation_0-auc:0.89048\n",
      "[88]\tvalidation_0-auc:0.89030\n",
      "[89]\tvalidation_0-auc:0.89079\n",
      "[90]\tvalidation_0-auc:0.89118\n",
      "[91]\tvalidation_0-auc:0.89100\n",
      "[92]\tvalidation_0-auc:0.89131\n",
      "[93]\tvalidation_0-auc:0.89100\n",
      "[94]\tvalidation_0-auc:0.89228\n",
      "[95]\tvalidation_0-auc:0.89177\n",
      "[96]\tvalidation_0-auc:0.89128\n",
      "[97]\tvalidation_0-auc:0.89177\n",
      "[98]\tvalidation_0-auc:0.89226\n",
      "[99]\tvalidation_0-auc:0.89238\n",
      "[0]\tvalidation_0-auc:0.82288\n",
      "[1]\tvalidation_0-auc:0.86886\n",
      "[2]\tvalidation_0-auc:0.86559\n",
      "[3]\tvalidation_0-auc:0.87600\n",
      "[4]\tvalidation_0-auc:0.85487\n",
      "[5]\tvalidation_0-auc:0.85156\n",
      "[6]\tvalidation_0-auc:0.85811\n",
      "[7]\tvalidation_0-auc:0.86250\n",
      "[8]\tvalidation_0-auc:0.86155\n",
      "[9]\tvalidation_0-auc:0.85413\n",
      "[10]\tvalidation_0-auc:0.84702\n",
      "[11]\tvalidation_0-auc:0.84144\n",
      "[12]\tvalidation_0-auc:0.85274\n",
      "[13]\tvalidation_0-auc:0.85801\n",
      "[14]\tvalidation_0-auc:0.86498\n",
      "[15]\tvalidation_0-auc:0.86870\n",
      "[16]\tvalidation_0-auc:0.87080\n",
      "[17]\tvalidation_0-auc:0.87499\n",
      "[18]\tvalidation_0-auc:0.87012\n",
      "[19]\tvalidation_0-auc:0.86730\n",
      "[20]\tvalidation_0-auc:0.86713\n",
      "[21]\tvalidation_0-auc:0.86880\n",
      "[22]\tvalidation_0-auc:0.87005\n",
      "[23]\tvalidation_0-auc:0.87131\n",
      "[24]\tvalidation_0-auc:0.87439\n",
      "[25]\tvalidation_0-auc:0.87360\n",
      "[26]\tvalidation_0-auc:0.87518\n",
      "[27]\tvalidation_0-auc:0.87359\n",
      "[28]\tvalidation_0-auc:0.87242\n",
      "[29]\tvalidation_0-auc:0.87438\n",
      "[30]\tvalidation_0-auc:0.87371\n",
      "[31]\tvalidation_0-auc:0.87389\n",
      "[32]\tvalidation_0-auc:0.87352\n",
      "[0]\tvalidation_0-auc:0.78655\n",
      "[1]\tvalidation_0-auc:0.80885\n",
      "[2]\tvalidation_0-auc:0.84923\n",
      "[3]\tvalidation_0-auc:0.86265\n",
      "[4]\tvalidation_0-auc:0.87576\n",
      "[5]\tvalidation_0-auc:0.87895\n",
      "[6]\tvalidation_0-auc:0.87699\n",
      "[7]\tvalidation_0-auc:0.87522\n",
      "[8]\tvalidation_0-auc:0.87321\n",
      "[9]\tvalidation_0-auc:0.87086\n",
      "[10]\tvalidation_0-auc:0.87121\n",
      "[11]\tvalidation_0-auc:0.87200\n",
      "[12]\tvalidation_0-auc:0.87556\n",
      "[13]\tvalidation_0-auc:0.87957\n",
      "[14]\tvalidation_0-auc:0.88071\n",
      "[15]\tvalidation_0-auc:0.87893\n",
      "[16]\tvalidation_0-auc:0.87755\n",
      "[17]\tvalidation_0-auc:0.88394\n",
      "[18]\tvalidation_0-auc:0.88373\n",
      "[19]\tvalidation_0-auc:0.88501\n",
      "[20]\tvalidation_0-auc:0.88392\n",
      "[21]\tvalidation_0-auc:0.88360\n",
      "[22]\tvalidation_0-auc:0.88393\n",
      "[23]\tvalidation_0-auc:0.88644\n",
      "[24]\tvalidation_0-auc:0.88636\n",
      "[25]\tvalidation_0-auc:0.88392\n",
      "[26]\tvalidation_0-auc:0.88842\n",
      "[27]\tvalidation_0-auc:0.88958\n",
      "[28]\tvalidation_0-auc:0.88977\n",
      "[29]\tvalidation_0-auc:0.88508\n",
      "[30]\tvalidation_0-auc:0.88665\n",
      "[31]\tvalidation_0-auc:0.88666\n",
      "[32]\tvalidation_0-auc:0.88716\n",
      "[33]\tvalidation_0-auc:0.88754\n",
      "[34]\tvalidation_0-auc:0.88676\n",
      "[35]\tvalidation_0-auc:0.88626\n",
      "[36]\tvalidation_0-auc:0.88667\n",
      "[37]\tvalidation_0-auc:0.88913\n",
      "[38]\tvalidation_0-auc:0.88675\n",
      "[39]\tvalidation_0-auc:0.88693\n",
      "[40]\tvalidation_0-auc:0.88517\n",
      "[41]\tvalidation_0-auc:0.88528\n",
      "[42]\tvalidation_0-auc:0.88309\n",
      "[43]\tvalidation_0-auc:0.88389\n",
      "[44]\tvalidation_0-auc:0.88318\n",
      "[45]\tvalidation_0-auc:0.88404\n",
      "[46]\tvalidation_0-auc:0.88435\n",
      "[47]\tvalidation_0-auc:0.88414\n",
      "[48]\tvalidation_0-auc:0.88395\n",
      "[49]\tvalidation_0-auc:0.88443\n",
      "[50]\tvalidation_0-auc:0.88504\n",
      "[51]\tvalidation_0-auc:0.88513\n",
      "[52]\tvalidation_0-auc:0.88410\n",
      "[53]\tvalidation_0-auc:0.88489\n",
      "[54]\tvalidation_0-auc:0.88527\n",
      "[55]\tvalidation_0-auc:0.88484\n",
      "[56]\tvalidation_0-auc:0.88503\n",
      "[57]\tvalidation_0-auc:0.88473\n",
      "[0]\tvalidation_0-auc:0.86482\n",
      "[1]\tvalidation_0-auc:0.90301\n",
      "[2]\tvalidation_0-auc:0.87143\n",
      "[3]\tvalidation_0-auc:0.86905\n",
      "[4]\tvalidation_0-auc:0.87245\n",
      "[5]\tvalidation_0-auc:0.86394\n",
      "[6]\tvalidation_0-auc:0.87038\n",
      "[7]\tvalidation_0-auc:0.86635\n",
      "[8]\tvalidation_0-auc:0.86753\n",
      "[9]\tvalidation_0-auc:0.86806\n",
      "[10]\tvalidation_0-auc:0.87140\n",
      "[11]\tvalidation_0-auc:0.87014\n",
      "[12]\tvalidation_0-auc:0.87422\n",
      "[13]\tvalidation_0-auc:0.87134\n",
      "[14]\tvalidation_0-auc:0.87304\n",
      "[15]\tvalidation_0-auc:0.87304\n",
      "[16]\tvalidation_0-auc:0.86506\n",
      "[17]\tvalidation_0-auc:0.86729\n",
      "[18]\tvalidation_0-auc:0.87252\n",
      "[19]\tvalidation_0-auc:0.87020\n",
      "[20]\tvalidation_0-auc:0.87190\n",
      "[21]\tvalidation_0-auc:0.86785\n",
      "[22]\tvalidation_0-auc:0.86559\n",
      "[23]\tvalidation_0-auc:0.86958\n",
      "[24]\tvalidation_0-auc:0.86958\n",
      "[25]\tvalidation_0-auc:0.87190\n",
      "[26]\tvalidation_0-auc:0.86844\n",
      "[27]\tvalidation_0-auc:0.86788\n",
      "[28]\tvalidation_0-auc:0.86955\n",
      "[29]\tvalidation_0-auc:0.87069\n",
      "[30]\tvalidation_0-auc:0.87187\n",
      "[31]\tvalidation_0-auc:0.86955\n"
     ]
    }
   ],
   "source": [
    "# 5 fold training with dataset balancing\n",
    "\n",
    "# (2207261730, 2983789) 0.784\n",
    "# (43987334, 39623)  0.809\n",
    "\n",
    "fold_clf_list = []\n",
    "\n",
    "for i in range(1, fold):\n",
    "    # 当前i下标的datafarme为validation set，其余为train set\n",
    "    # fold的最后一个下标为test set\n",
    "    \n",
    "    df_val = pd.DataFrame(columns=df_variant.columns)\n",
    "    part_dict = defaultdict(list)\n",
    "    \n",
    "    for key, value in function_dict.items():    \n",
    "        for j in range(1, fold):\n",
    "            if i == j:\n",
    "                df_val = pd.concat([df_val, value[j]], axis=0)\n",
    "            else:\n",
    "                part_dict[key].append(value[j])\n",
    "    \n",
    "    # balancing traning set\n",
    "    for key in part_dict.keys():\n",
    "        part_dict[key] = pd.concat(part_dict[key], axis=0)\n",
    "    \n",
    "    # upsampling with 2 * max_len\n",
    "    max_len = max([len(x) for x in part_dict.values()])\n",
    "    for key in part_dict.keys():\n",
    "        part_dict[key] = part_dict[key].sample(\n",
    "            n=max_len*2, replace=True, random_state=seed2)\n",
    "        \n",
    "    df_train = pd.concat(list(part_dict.values()), axis=0)\n",
    "\n",
    "    train_label = df_train[\"function\"].values\n",
    "    val_label = df_val[\"function\"].values\n",
    "    \n",
    "    for col in [\"gene\", \"haplotype_name\", \"chr\", \"variant_start\", \n",
    "                \"reference_allele\", \"variant_allele\", \"function\", \n",
    "                \"variant\", \"type\"]:\n",
    "        df_train.pop(col)\n",
    "        df_val.pop(col)\n",
    "        \n",
    "    train_data = df_train.values\n",
    "    val_data = df_val.values\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "        learning_rate=0.005,\n",
    "        subsample=0.6,\n",
    "        max_depth=5,\n",
    "        n_estimators=100,\n",
    "    )\n",
    "    \n",
    "    clf.fit(\n",
    "        train_data, train_label,\n",
    "        eval_metric='auc',\n",
    "        eval_set=[(val_data, val_label)],\n",
    "        early_stopping_rounds=30\n",
    "    )\n",
    "        \n",
    "    fold_clf_list.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "551ebf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(true_label, pred_label, classes):\n",
    "    con_mat = confusion_matrix(true_label, pred_label)\n",
    "\n",
    "    sensitivity_list = []\n",
    "    for i in range(con_mat.shape[0]):\n",
    "        tp = con_mat[i][i]\n",
    "        fn = np.sum(con_mat[i,:]) - tp\n",
    "        sensitivity = round(tp / (tp + fn), 4)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "\n",
    "    class_sensitivity = dict(zip(classes, sensitivity_list))\n",
    "    print(\"sensitivity: \", class_sensitivity)\n",
    "    print(\"avg sensitivity: \", round(sum(sensitivity_list) / con_mat.shape[0], 4))\n",
    "    print()\n",
    "        \n",
    "    precision_list = []\n",
    "    for i in range(con_mat.shape[0]):\n",
    "        tp = con_mat[i][i]\n",
    "        fp = np.sum(con_mat[:,i]) - tp\n",
    "        precision = round(tp / (tp + fp), 4)\n",
    "        precision_list.append(precision)\n",
    "        \n",
    "    class_precision = dict(zip(classes, precision_list))\n",
    "    print(\"precision: \", class_precision)\n",
    "    print(\"avg precision: \", round(sum(precision_list) / con_mat.shape[0], 4))\n",
    "    print()\n",
    "    \n",
    "    f1_list = [round((x[0] * x[1] * 2) / (x[0] + x[1]), 4) for x in zip(sensitivity_list, precision_list)]\n",
    "    class_f1 = dict(zip(classes, f1_list))\n",
    "    print(\"f1: \", class_f1)\n",
    "    print(\"avg f1: \", round(sum(f1_list) / con_mat.shape[0], 4))\n",
    "    print()\n",
    "    \n",
    "    specificity_list = []\n",
    "    for i in range(con_mat.shape[0]):\n",
    "        number = np.sum(con_mat[:,:])\n",
    "        tp = con_mat[i][i]\n",
    "        fn = np.sum(con_mat[i,:]) - tp\n",
    "        fp = np.sum(con_mat[:,i]) - tp\n",
    "        tn = number - tp - fn - fp\n",
    "        specificity = round(tn / (tn + fp), 4)\n",
    "        specificity_list.append(specificity)\n",
    "        \n",
    "    class_specificity = dict(zip(classes, specificity_list))\n",
    "    print(\"specificity: \", class_specificity)\n",
    "    print(\"avg specificity: \", round(sum(specificity_list) / con_mat.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf2df13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.6154, 'increased function': 1.0, 'no function': 0.8438, 'normal function': 0.8182}\n",
      "avg sensitivity:  0.8194\n",
      "\n",
      "precision:  {'decreased function': 0.7273, 'increased function': 1.0, 'no function': 0.871, 'normal function': 0.6429}\n",
      "avg precision:  0.8103\n",
      "\n",
      "f1:  {'decreased function': 0.6667, 'increased function': 1.0, 'no function': 0.8572, 'normal function': 0.72}\n",
      "avg f1:  0.811\n",
      "\n",
      "specificity:  {'decreased function': 0.9412, 'increased function': 1.0, 'no function': 0.875, 'normal function': 0.9057}\n",
      "avg specificity:  0.9305\n",
      "precision: 0.8125\n",
      "sensitivity:  {'decreased function': 0.6923, 'increased function': 0.875, 'no function': 0.7812, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.8144\n",
      "\n",
      "precision:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.8621, 'normal function': 0.6667}\n",
      "avg precision:  0.8053\n",
      "\n",
      "f1:  {'decreased function': 0.6923, 'increased function': 0.9333, 'no function': 0.8197, 'normal function': 0.7693}\n",
      "avg f1:  0.8036\n",
      "\n",
      "specificity:  {'decreased function': 0.9216, 'increased function': 1.0, 'no function': 0.875, 'normal function': 0.9057}\n",
      "avg specificity:  0.9256\n",
      "precision: 0.796875\n",
      "sensitivity:  {'decreased function': 0.7692, 'increased function': 1.0, 'no function': 0.7812, 'normal function': 0.8182}\n",
      "avg sensitivity:  0.8422\n",
      "\n",
      "precision:  {'decreased function': 0.625, 'increased function': 1.0, 'no function': 0.9615, 'normal function': 0.6429}\n",
      "avg precision:  0.8074\n",
      "\n",
      "f1:  {'decreased function': 0.6896, 'increased function': 1.0, 'no function': 0.862, 'normal function': 0.72}\n",
      "avg f1:  0.8179\n",
      "\n",
      "specificity:  {'decreased function': 0.8824, 'increased function': 1.0, 'no function': 0.9688, 'normal function': 0.9057}\n",
      "avg specificity:  0.9392\n",
      "precision: 0.8125\n",
      "sensitivity:  {'decreased function': 0.7692, 'increased function': 1.0, 'no function': 0.7188, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.8493\n",
      "\n",
      "precision:  {'decreased function': 0.7143, 'increased function': 1.0, 'no function': 0.8846, 'normal function': 0.625}\n",
      "avg precision:  0.806\n",
      "\n",
      "f1:  {'decreased function': 0.7407, 'increased function': 1.0, 'no function': 0.7931, 'normal function': 0.7407}\n",
      "avg f1:  0.8186\n",
      "\n",
      "specificity:  {'decreased function': 0.9216, 'increased function': 1.0, 'no function': 0.9062, 'normal function': 0.8868}\n",
      "avg specificity:  0.9286\n",
      "precision: 0.796875\n",
      "sensitivity:  {'decreased function': 0.7692, 'increased function': 1.0, 'no function': 0.7812, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.8649\n",
      "\n",
      "precision:  {'decreased function': 0.7143, 'increased function': 1.0, 'no function': 0.9259, 'normal function': 0.6667}\n",
      "avg precision:  0.8267\n",
      "\n",
      "f1:  {'decreased function': 0.7407, 'increased function': 1.0, 'no function': 0.8474, 'normal function': 0.7693}\n",
      "avg f1:  0.8394\n",
      "\n",
      "specificity:  {'decreased function': 0.9216, 'increased function': 1.0, 'no function': 0.9375, 'normal function': 0.9057}\n",
      "avg specificity:  0.9412\n",
      "precision: 0.828125\n",
      "average precision: 0.809375\n"
     ]
    }
   ],
   "source": [
    "test_list = []\n",
    "for key, value in function_dict.items():\n",
    "    test_list.append(value[0])\n",
    "    \n",
    "df_test = pd.concat(test_list, axis=0)\n",
    "df_test_ = df_test.copy(deep=True)\n",
    "\n",
    "test_label = df_test[\"function\"].values\n",
    "for col in [\"gene\", \"haplotype_name\", \"chr\", \"variant_start\", \n",
    "                \"reference_allele\", \"variant_allele\", \"function\", \n",
    "                \"variant\", \"type\"]:\n",
    "    df_test_.pop(col)\n",
    "\n",
    "test_data = df_test_.values\n",
    "    \n",
    "average_precision = 0\n",
    "for clf in fold_clf_list:\n",
    "    result = list(clf.predict(test_data))\n",
    "    performance(test_label, result, classes=clf.classes_)\n",
    "    precision = sum([1 if test_label[i] == result[i] else 0 for i in range(len(result))]) / len(result)\n",
    "    average_precision += precision\n",
    "    print(\"precision: {}\".format(precision))\n",
    "    \n",
    "print(\"average precision: {}\".format(average_precision / len(fold_clf_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a966f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# columns = df_test_.columns\n",
    "\n",
    "# # 模型特征分相加\n",
    "# feature_score = np.zeros(len(columns))\n",
    "# for clf in fold_clf_list:\n",
    "#     feature_score = feature_score + clf.feature_importances_\n",
    "\n",
    "# sorted_idx = np.argsort(feature_score)[::-1]\n",
    "\n",
    "# for index in sorted_idx[:40]:\n",
    "#     print([columns[index], feature_score[index]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c329b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total set training\n",
    "part_dict = defaultdict(list)\n",
    "for key, value in function_dict.items():    \n",
    "    for j in range(1, fold):\n",
    "        part_dict[key].append(value[j])\n",
    "        \n",
    "# balancing traning set\n",
    "for key in part_dict.keys():\n",
    "    part_dict[key] = pd.concat(part_dict[key], axis=0)\n",
    "\n",
    "# upsampling with 2 * max_len\n",
    "max_len = max([len(x) for x in part_dict.values()])\n",
    "for key in part_dict.keys():\n",
    "    part_dict[key] = part_dict[key].sample(\n",
    "        n=max_len*2, replace=True, random_state=seed2)\n",
    "\n",
    "df_train = pd.concat(list(part_dict.values()), axis=0)\n",
    "\n",
    "train_label = df_train[\"function\"].values\n",
    "\n",
    "for col in [\"gene\", \"haplotype_name\", \"chr\", \"variant_start\", \n",
    "                \"reference_allele\", \"variant_allele\", \"function\", \n",
    "                \"variant\", \"type\"]:\n",
    "    df_train.pop(col)\n",
    "    \n",
    "df_train_ = df_train.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "410d8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_key = df_train_.copy(deep=True)\n",
    "col_list = [\n",
    "        'DEOGEN2_score', 'M-CAP_score', 'MPC_score', 'MutationAssessor_score',\n",
    "        'LRT_score', 'FATHMM_score', 'PROVEAN_score',\n",
    "        'Polyphen2_HVAR_score', 'integrated_fitCons_score', 'VEST4_score',\n",
    "        'SIFT4G_score', 'LoFtool', 'GenoCanyon_score', 'CADD_raw', 'APF_score'\n",
    "    ]\n",
    "for col in df_train_key.columns:\n",
    "    if \"_variant\" in col or \"_gained\" in col or \"_lost\" in col:\n",
    "        col_list.append(col)\n",
    "        \n",
    "df_train_all_ = df_train_key[col_list]\n",
    "\n",
    "# 使用空值数据\n",
    "# df_train_all_ = pd.concat([df_train_, df_train_key], axis=0)\n",
    "\n",
    "# 不使用空值数据\n",
    "# df_train_all_ = df_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82ebe7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_dict = {}\n",
    "normalizer_dict = {}\n",
    "for col in df_train.columns:\n",
    "    if \"_variant\" in col or \"_gained\" in col or \"_lost\" in col:\n",
    "        continue\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean').fit(df_train[col].values.reshape(-1, 1))\n",
    "    imputer_dict[col] = imp\n",
    "    df_train[col] = imp.transform(df_train[col].values.reshape(-1, 1))\n",
    "    \n",
    "    normalizer = StandardScaler().fit(df_train[col].values.reshape(-1, 1))\n",
    "    normalizer_dict[col] = normalizer\n",
    "    df_train[col] = normalizer.transform(df_train[col].values.reshape(-1, 1))\n",
    "    \n",
    "with open(\"model/imputer_dict_multi.pkl\", \"wb\") as f:\n",
    "    pickle.dump(imputer_dict, f)\n",
    "    \n",
    "with open(\"model/normalizer_dict_multi.pkl\", \"wb\") as f:\n",
    "    pickle.dump(normalizer_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c627ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_key = df_train.copy(deep=True)\n",
    "for col in df_train_key.columns:\n",
    "    if \"_variant\" in col or \"_gained\" in col or \"_lost\" in col:\n",
    "        continue\n",
    "        \n",
    "    if col not in [\n",
    "        'DEOGEN2_score', 'M-CAP_score', 'MPC_score', 'MutationAssessor_score',\n",
    "        'LRT_score', 'FATHMM_score', 'PROVEAN_score',\n",
    "        'Polyphen2_HVAR_score', 'integrated_fitCons_score', 'VEST4_score',\n",
    "        'SIFT4G_score', 'LoFtool', 'GenoCanyon_score', 'CADD_raw', 'APF_score'\n",
    "    ]:\n",
    "        df_train_key[col] = [np.nan] * len(df_train_key)\n",
    "        df_train_key[col] = imputer_dict[col].transform(df_train_key[col].values.reshape(-1, 1))\n",
    "        df_train_key[col] = normalizer_dict[col].transform(df_train_key[col].values.reshape(-1, 1))\n",
    "        \n",
    "# 使用空值数据\n",
    "# df_train_all = pd.concat([df_train, df_train_key], axis=0)\n",
    "\n",
    "# 不使用空值数据\n",
    "df_train_all = df_train_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f54ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data generation\n",
    "test_list = []\n",
    "for key, value in function_dict.items():\n",
    "    test_list.append(value[0])\n",
    "    \n",
    "df_test = pd.concat(test_list, axis=0)\n",
    "df_test_ = df_test.copy(deep=True)\n",
    "\n",
    "test_label = df_test[\"function\"].values\n",
    "for col in [\"gene\", \"haplotype_name\", \"chr\", \"variant_start\", \n",
    "                \"reference_allele\", \"variant_allele\", \"function\", \n",
    "                \"variant\", \"type\"]:\n",
    "    df_test_.pop(col)\n",
    "\n",
    "df_test_fillna = df_test_.copy(deep=True)\n",
    "\n",
    "df_test_ = df_test_[col_list]\n",
    "\n",
    "with open(\"model/imputer_dict_multi.pkl\", \"rb\") as f:\n",
    "    imputer_dict = pickle.load(f)\n",
    "    \n",
    "with open(\"model/normalizer_dict_multi.pkl\", \"rb\") as f:\n",
    "    normalizer_dict = pickle.load(f)\n",
    "    \n",
    "\n",
    "    \n",
    "for key in imputer_dict.keys():\n",
    "    df_test_fillna[key] = imputer_dict[key].transform(df_test_fillna[key].values.reshape(-1, 1))\n",
    "    df_test_fillna[key] = normalizer_dict[key].transform(df_test_fillna[key].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "315baa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_train_all_.values\n",
    "test_data = df_test_.values\n",
    "\n",
    "train_data_fillna = df_train_all.values\n",
    "test_data_fillna = df_test_fillna.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4842eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1fda9c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\drkg\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.7188, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.83\n",
      "\n",
      "precision:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.92, 'normal function': 0.5556}\n",
      "avg precision:  0.792\n",
      "\n",
      "f1:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.807, 'normal function': 0.6897}\n",
      "avg f1:  0.7972\n",
      "\n",
      "specificity:  {'decreased function': 0.9216, 'increased function': 1.0, 'no function': 0.9375, 'normal function': 0.8491}\n",
      "avg specificity:  0.927\n"
     ]
    }
   ],
   "source": [
    "# xgboost model\n",
    "xgb_model = XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "        subsample=0.5,\n",
    "        max_depth=4,\n",
    "        n_estimators=100,\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    train_data, list(train_label),\n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "classes = xgb_model.classes_\n",
    "y_pred = xgb_model.predict_proba(test_data)\n",
    "result = xgb_model.predict(test_data)\n",
    "\n",
    "xgb_model.voting = {\n",
    "    \"classes\": classes,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"result\": result\n",
    "}\n",
    "\n",
    "performance(test_label, result, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d38a35b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.8438, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.8613\n",
      "\n",
      "precision:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.871, 'normal function': 0.8333}\n",
      "avg precision:  0.8492\n",
      "\n",
      "f1:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.8572, 'normal function': 0.8696}\n",
      "avg f1:  0.8548\n",
      "\n",
      "specificity:  {'decreased function': 0.9216, 'increased function': 1.0, 'no function': 0.875, 'normal function': 0.9623}\n",
      "avg specificity:  0.9397\n"
     ]
    }
   ],
   "source": [
    "# lightgbm model\n",
    "lgb_model = LGBMClassifier(\n",
    "    max_depth=4, \n",
    "    learning_rate=0.09, \n",
    "    n_estimators=100,\n",
    "    subsample=0.6, \n",
    "    reg_lambda=0.1\n",
    ")\n",
    "\n",
    "lgb_model.fit(\n",
    "   train_data, list(train_label),\n",
    ")\n",
    "y_pred = lgb_model.predict_proba(test_data)\n",
    "classes = lgb_model.classes_\n",
    "result = lgb_model.predict(test_data)\n",
    "\n",
    "lgb_model.voting = {\n",
    "    \"classes\": classes,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"result\": result\n",
    "}\n",
    "\n",
    "performance(test_label, result, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f55006f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.2308, 'increased function': 1.0, 'no function': 0.75, 'normal function': 0.7273}\n",
      "avg sensitivity:  0.677\n",
      "\n",
      "precision:  {'decreased function': 0.6, 'increased function': 0.8889, 'no function': 0.6857, 'normal function': 0.5333}\n",
      "avg precision:  0.677\n",
      "\n",
      "f1:  {'decreased function': 0.3334, 'increased function': 0.9412, 'no function': 0.7164, 'normal function': 0.6154}\n",
      "avg f1:  0.6516\n",
      "\n",
      "specificity:  {'decreased function': 0.9608, 'increased function': 0.9821, 'no function': 0.6562, 'normal function': 0.8679}\n",
      "avg specificity:  0.8668\n"
     ]
    }
   ],
   "source": [
    "# rf model \n",
    "rf_model = RandomForestClassifier(\n",
    "    max_depth=5,\n",
    "    random_state=220\n",
    ")\n",
    "\n",
    "rf_model.fit(\n",
    "   train_data_fillna, list(train_label),\n",
    ")\n",
    "\n",
    "classes = rf_model.classes_\n",
    "y_pred = rf_model.predict_proba(test_data_fillna)\n",
    "result = rf_model.predict(test_data_fillna)\n",
    "\n",
    "rf_model.voting = {\n",
    "    \"classes\": classes,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"result\": result\n",
    "}\n",
    "\n",
    "performance(test_label, result, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78870b59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.5385, 'increased function': 0.0, 'no function': 0.5625, 'normal function': 0.0909}\n",
      "avg sensitivity:  0.298\n",
      "\n",
      "precision:  {'decreased function': 0.25, 'increased function': 0.0, 'no function': 0.5806, 'normal function': 0.5}\n",
      "avg precision:  0.3326\n",
      "\n",
      "f1:  {'decreased function': 0.3415, 'increased function': nan, 'no function': 0.5714, 'normal function': 0.1538}\n",
      "avg f1:  nan\n",
      "\n",
      "specificity:  {'decreased function': 0.5882, 'increased function': 0.9464, 'no function': 0.5938, 'normal function': 0.9811}\n",
      "avg specificity:  0.7774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\drkg\\lib\\site-packages\\ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# lr model\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    C=0.05\n",
    ")\n",
    "\n",
    "lr_model.fit(\n",
    "   train_data_fillna, list(train_label),\n",
    ")\n",
    "\n",
    "classes = lr_model.classes_\n",
    "y_pred = lr_model.predict_proba(test_data_fillna)\n",
    "result = lr_model.predict(test_data_fillna)\n",
    "\n",
    "lr_model.voting = {\n",
    "    \"classes\": classes,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"result\": result\n",
    "}\n",
    "\n",
    "performance(test_label, result, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "09037306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.6154, 'increased function': 1.0, 'no function': 0.7812, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.8264\n",
      "\n",
      "precision:  {'decreased function': 0.7273, 'increased function': 1.0, 'no function': 0.8929, 'normal function': 0.5882}\n",
      "avg precision:  0.8021\n",
      "\n",
      "f1:  {'decreased function': 0.6667, 'increased function': 1.0, 'no function': 0.8333, 'normal function': 0.7143}\n",
      "avg f1:  0.8036\n",
      "\n",
      "specificity:  {'decreased function': 0.9412, 'increased function': 1.0, 'no function': 0.9062, 'normal function': 0.8679}\n",
      "avg specificity:  0.9288\n"
     ]
    }
   ],
   "source": [
    "# hard voting\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(lst, key=lst.count)\n",
    "\n",
    "hard_vote = [\n",
    "    most_common(x) for x in list(zip(\n",
    "        xgb_model.voting[\"result\"],\n",
    "        lgb_model.voting[\"result\"],\n",
    "        rf_model.voting[\"result\"],\n",
    "#         lr_model.voting[\"result\"]\n",
    "    ))\n",
    "]\n",
    "\n",
    "performance(test_label, hard_vote, xgb_model.voting[\"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a432a4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.8125, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.8535\n",
      "\n",
      "precision:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.8667, 'normal function': 0.7692}\n",
      "avg precision:  0.832\n",
      "\n",
      "f1:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.8387, 'normal function': 0.8333}\n",
      "avg f1:  0.8411\n",
      "\n",
      "specificity:  {'decreased function': 0.9216, 'increased function': 1.0, 'no function': 0.875, 'normal function': 0.9434}\n",
      "avg specificity:  0.935\n"
     ]
    }
   ],
   "source": [
    "# 软投票\n",
    "y_pred = (\n",
    "    xgb_model.voting[\"y_pred\"] \n",
    "    + lgb_model.voting[\"y_pred\"] \n",
    "#     + rf_model.voting[\"y_pred\"] \n",
    "#     + lr_model.voting[\"y_pred\"] \n",
    ")\n",
    "soft_vote = [xgb_model.voting[\"classes\"][x] for x in np.argmax(y_pred, axis=1)]\n",
    "\n",
    "performance(test_label, soft_vote, xgb_model.voting[\"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c5a97fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "today_str = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "def save_model():\n",
    "    with open(\"model/xgb_multi_{}.pkl\".format(today_str), \"wb\") as f:\n",
    "        pickle.dump(xgb_model, f)\n",
    "    \n",
    "    with open(\"model/lgb_multi_{}.pkl\".format(today_str), \"wb\") as f:\n",
    "        pickle.dump(lgb_model, f)\n",
    "    \n",
    "    with open(\"model/rf_multi_{}.pkl\".format(today_str), \"wb\") as f:\n",
    "        pickle.dump(rf_model, f)\n",
    "        \n",
    "    with open(\"model/lr_multi_{}.pkl\".format(today_str), \"wb\") as f:\n",
    "        pickle.dump(lr_model, f)\n",
    "        \n",
    "    with open(\"model/col_list_multi_{}.pkl\".format(today_str), \"wb\") as f:\n",
    "        pickle.dump(col_list, f)\n",
    "        \n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20219cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_show = df_test[[\"gene\", \"haplotype_name\", \"chr\", \"variant_start\", \"reference_allele\", \"variant_allele\", \"function\"]]\n",
    "# df_test_show.index = range(len(df_test_show))\n",
    "# df_test_show[\"function_prediction\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ecbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "446f871d",
   "metadata": {},
   "source": [
    "# test missing feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1536a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"model/imputer_dict_multi.pkl\", \"rb\") as f:\n",
    "#     imputer_dict = pickle.load(f)\n",
    "\n",
    "# with open(\"model/normalizer_dict_multi.pkl\", \"rb\") as f:\n",
    "#     normalizer_dict = pickle.load(f)\n",
    "\n",
    "# with open(\"model/xgb_multi_0823.pkl\", \"rb\") as f:\n",
    "#     xgb_model = pickle.load(f)\n",
    "    \n",
    "# with open(\"model/lgb_multi_0823.pkl\", \"rb\") as f:\n",
    "#     lgb_model = pickle.load(f)\n",
    "\n",
    "# with open(\"model/rf_multi_0823.pkl\", \"rb\") as f:\n",
    "#     rf_model = pickle.load( f)\n",
    "\n",
    "# with open(\"model/lr_multi_0823.pkl\", \"rb\") as f:\n",
    "#     lr_model = pickle.load( f)\n",
    "    \n",
    "\n",
    "df_test_key = df_test_.copy(deep=True)\n",
    "for col in df_test_key.columns:\n",
    "    if \"_variant\" in col or \"_gained\" in col or \"_lost\" in col:\n",
    "        continue\n",
    "        \n",
    "    if col not in [\n",
    "        'DEOGEN2_score', 'M-CAP_score', 'MPC_score', 'MutationAssessor_score',\n",
    "        'LRT_score', 'FATHMM_score', 'PROVEAN_score',\n",
    "        'Polyphen2_HVAR_score', 'integrated_fitCons_score', 'VEST4_score',\n",
    "        'SIFT4G_score', 'LoFtool', 'GenoCanyon_score', 'CADD_raw', 'APF_score'\n",
    "    ]:\n",
    "        df_test_key[col] = [np.nan] * len(df_test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12c52811",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_key_fillna = df_test_key.copy(deep=True)\n",
    "    \n",
    "for key in imputer_dict.keys():\n",
    "    df_test_key_fillna[key] = imputer_dict[key].transform(df_test_key_fillna[key].values.reshape(-1, 1))\n",
    "    df_test_key_fillna[key] = normalizer_dict[key].transform(df_test_key_fillna[key].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45714111",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_test_data = df_test_key.values\n",
    "key_test_data_fillna = df_test_key_fillna.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d0babbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.6154, 'increased function': 1.0, 'no function': 0.8438, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.8421\n",
      "\n",
      "precision:  {'decreased function': 0.6667, 'increased function': 1.0, 'no function': 0.871, 'normal function': 0.7692}\n",
      "avg precision:  0.8267\n",
      "\n",
      "f1:  {'decreased function': 0.64, 'increased function': 1.0, 'no function': 0.8572, 'normal function': 0.8333}\n",
      "avg f1:  0.8326\n",
      "\n",
      "specificity:  {'decreased function': 0.9216, 'increased function': 1.0, 'no function': 0.875, 'normal function': 0.9434}\n",
      "avg specificity:  0.935\n"
     ]
    }
   ],
   "source": [
    "y_pred = lgb_model.predict_proba(key_test_data)\n",
    "classes = lgb_model.classes_\n",
    "result = lgb_model.predict(key_test_data)\n",
    "\n",
    "lgb_model.voting = {\n",
    "    \"classes\": classes,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"result\": result\n",
    "}\n",
    "\n",
    "performance(test_label, result, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b1f4b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.7692, 'increased function': 1.0, 'no function': 0.7188, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.8493\n",
      "\n",
      "precision:  {'decreased function': 0.6667, 'increased function': 1.0, 'no function': 0.92, 'normal function': 0.625}\n",
      "avg precision:  0.8029\n",
      "\n",
      "f1:  {'decreased function': 0.7143, 'increased function': 1.0, 'no function': 0.807, 'normal function': 0.7407}\n",
      "avg f1:  0.8155\n",
      "\n",
      "specificity:  {'decreased function': 0.902, 'increased function': 1.0, 'no function': 0.9375, 'normal function': 0.8868}\n",
      "avg specificity:  0.9316\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_model.predict_proba(key_test_data)\n",
    "classes = xgb_model.classes_\n",
    "result = xgb_model.predict(key_test_data)\n",
    "\n",
    "xgb_model.voting = {\n",
    "    \"classes\": classes,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"result\": result\n",
    "}\n",
    "\n",
    "performance(test_label, result, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85ef25d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.6923, 'increased function': 1.0, 'no function': 0.6562, 'normal function': 0.7273}\n",
      "avg sensitivity:  0.769\n",
      "\n",
      "precision:  {'decreased function': 0.6923, 'increased function': 0.5, 'no function': 0.875, 'normal function': 0.7273}\n",
      "avg precision:  0.6986\n",
      "\n",
      "f1:  {'decreased function': 0.6923, 'increased function': 0.6667, 'no function': 0.75, 'normal function': 0.7273}\n",
      "avg f1:  0.7091\n",
      "\n",
      "specificity:  {'decreased function': 0.9216, 'increased function': 0.8571, 'no function': 0.9062, 'normal function': 0.9434}\n",
      "avg specificity:  0.9071\n"
     ]
    }
   ],
   "source": [
    "classes = lr_model.classes_\n",
    "y_pred = lr_model.predict_proba(key_test_data_fillna)\n",
    "result = lr_model.predict(key_test_data_fillna)\n",
    "\n",
    "lr_model.voting = {\n",
    "    \"classes\": classes,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"result\": result\n",
    "}\n",
    "\n",
    "performance(test_label, result, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c04c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.5385, 'increased function': 1.0, 'no function': 0.8125, 'normal function': 0.8182}\n",
      "avg sensitivity:  0.7923\n",
      "\n",
      "precision:  {'decreased function': 0.7, 'increased function': 0.7273, 'no function': 0.8387, 'normal function': 0.75}\n",
      "avg precision:  0.754\n",
      "\n",
      "f1:  {'decreased function': 0.6087, 'increased function': 0.8421, 'no function': 0.8254, 'normal function': 0.7826}\n",
      "avg f1:  0.7647\n",
      "\n",
      "specificity:  {'decreased function': 0.9412, 'increased function': 0.9464, 'no function': 0.8438, 'normal function': 0.9434}\n",
      "avg specificity:  0.9187\n"
     ]
    }
   ],
   "source": [
    "classes = rf_model.classes_\n",
    "y_pred = rf_model.predict_proba(key_test_data_fillna)\n",
    "result = rf_model.predict(key_test_data_fillna)\n",
    "\n",
    "rf_model.voting = {\n",
    "    \"classes\": classes,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"result\": result\n",
    "}\n",
    "\n",
    "performance(test_label, result, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e219aed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.6154, 'increased function': 1.0, 'no function': 0.8438, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.8421\n",
      "\n",
      "precision:  {'decreased function': 0.6667, 'increased function': 1.0, 'no function': 0.871, 'normal function': 0.7692}\n",
      "avg precision:  0.8267\n",
      "\n",
      "f1:  {'decreased function': 0.64, 'increased function': 1.0, 'no function': 0.8572, 'normal function': 0.8333}\n",
      "avg f1:  0.8326\n",
      "\n",
      "specificity:  {'decreased function': 0.9216, 'increased function': 1.0, 'no function': 0.875, 'normal function': 0.9434}\n",
      "avg specificity:  0.935\n"
     ]
    }
   ],
   "source": [
    "# hard voting\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(lst, key=lst.count)\n",
    "\n",
    "hard_vote = [\n",
    "    most_common(x) for x in list(zip(\n",
    "        lgb_model.voting[\"result\"],\n",
    "        xgb_model.voting[\"result\"],\n",
    "    ))\n",
    "]\n",
    "\n",
    "performance(test_label, hard_vote, lgb_model.voting[\"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a4f1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity:  {'decreased function': 0.6154, 'increased function': 1.0, 'no function': 0.8125, 'normal function': 0.9091}\n",
      "avg sensitivity:  0.8342\n",
      "\n",
      "precision:  {'decreased function': 0.6667, 'increased function': 1.0, 'no function': 0.8667, 'normal function': 0.7143}\n",
      "avg precision:  0.8119\n",
      "\n",
      "f1:  {'decreased function': 0.64, 'increased function': 1.0, 'no function': 0.8387, 'normal function': 0.8}\n",
      "avg f1:  0.8197\n",
      "\n",
      "specificity:  {'decreased function': 0.9216, 'increased function': 1.0, 'no function': 0.875, 'normal function': 0.9245}\n",
      "avg specificity:  0.9303\n"
     ]
    }
   ],
   "source": [
    "y_pred = (\n",
    "    xgb_model.voting[\"y_pred\"] \n",
    "    + lgb_model.voting[\"y_pred\"] \n",
    "    + rf_model.voting[\"y_pred\"] \n",
    "    + lr_model.voting[\"y_pred\"] \n",
    ")\n",
    "soft_vote = [xgb_model.voting[\"classes\"][x] for x in np.argmax(y_pred, axis=1)]\n",
    "\n",
    "performance(test_label, soft_vote, xgb_model.voting[\"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbfedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebda83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90982af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e09fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
