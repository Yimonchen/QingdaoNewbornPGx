{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a31740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "297dbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from xlearn import FMModel,LRModel\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, \\\n",
    "balanced_accuracy_score, roc_auc_score, auc, precision_recall_curve, \\\n",
    "roc_curve\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fc2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"D:/functional-prediction/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "030d9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_variant = pd.read_csv(\"output/variant_feat_clean.csv\")\n",
    "df_apf = pd.read_csv(\"output/apf_feat_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15f4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.concat([df_variant, df_apf], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579f2506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\drkg\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 去重\n",
    "df_feat[\"function\"] = [\"neutral\" if \"normal\" in x or x == \"N\" else \"deleterious\" for x in df_feat[\"function\"].values]\n",
    "df_feat[\"chr\"] = [str(x) for x in df_feat[\"chr\"].values]\n",
    "df_feat = df_feat.drop_duplicates([\"gene\", \"variant_start\", \"reference_allele\", \"variant_allele\", \"function\"])\n",
    "_df = df_feat.groupby(\n",
    "    [\"gene\", \"variant_start\", \"reference_allele\", \"variant_allele\"]\n",
    ")[\"function\"].count().reset_index(name='count').sort_values(by=[\"count\"], ascending=False)\n",
    "_df = _df[_df[\"count\"] == 2]\n",
    "_df[\"function_new\"] = [\"neutral\"] * len(_df)\n",
    "_df.pop('count')\n",
    "\n",
    "df_feat = pd.merge(df_feat, _df, how='left', on=[\"gene\", \"variant_start\", \"reference_allele\", \"variant_allele\"])\n",
    "df_dup = df_feat[df_feat[\"function_new\"].notnull()]\n",
    "df_dup[\"function\"] = [\"neutral\"] * len(df_dup)\n",
    "df_dup = df_dup.drop_duplicates([\"gene\", \"variant_start\", \"reference_allele\", \"variant_allele\", \"function\"])\n",
    "\n",
    "df_not_dup = df_feat[~df_feat[\"function_new\"].notnull()]\n",
    "df_feat = pd.concat([df_dup, df_not_dup], axis=0)\n",
    "df_feat.pop(\"function_new\")\n",
    "print(len(df_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e772f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deleterious</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      function  count\n",
       "0  deleterious    385\n",
       "1      neutral    150"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat.groupby([\"function\"])[\"gene\"].count().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b62a5982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCC2, CYP17A1, CYP1A2, CYP1B1, CYP21A2, CYP2A13, CYP2A6, CYP2B6, CYP2C19, CYP2C9, CYP2D6, DPYD, FMO3, G6PD, GSTP1, HNMT, NAT1, NAT2, NR1I2, NUDT15, PNMT, RYR1, SLC22A12, SLC2A9, SLC47A1, SLCO1B1, TPMT, UGT1A1, UGT1A10, UGT1A4, UGT1A8, UGT1A9'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = df_feat.groupby([\"gene\"])[\"function\"].count().reset_index(name='count')\n",
    "\", \".join(list(_df[\"gene\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8438db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed1 = 14398733\n",
    "seed2 = 1396232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70e16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold split\n",
    "# good seeds: (43987334， 3962321)\n",
    "function_dict = {}\n",
    "fold = 6 # 1 test set + 5 train-validation set \n",
    "\n",
    "for function, content in df_feat.groupby([\"function\"]):\n",
    "    content = content.sample(frac=1, random_state=seed1).reset_index(drop=True)\n",
    "\n",
    "    fold_size = int(len(content) / fold) + 1\n",
    "    \n",
    "    content_list = []\n",
    "    for i in range(fold):\n",
    "        content_part = content.iloc[i * fold_size: (i + 1) * fold_size]\n",
    "        content_part.index = range(len(content_part))\n",
    "        content_list.append(content_part)\n",
    "    \n",
    "    function_dict[function] = content_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631cb81f",
   "metadata": {},
   "source": [
    "# get test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e67c47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "test_partition = 5\n",
    "\n",
    "for key, value in function_dict.items():\n",
    "    test_list.append(value[test_partition])\n",
    "    \n",
    "df_test = pd.concat(test_list, axis=0)\n",
    "df_test_ = df_test.copy(deep=True)\n",
    "\n",
    "test_label = df_test[\"function\"].values\n",
    "test_variant_start = df_test[\"variant_start\"].values\n",
    "test_apf = df_test[\"APF_score\"].values\n",
    "for col in [\"gene\", \"haplotype_name\", \"chr\", \"variant_start\", \n",
    "                \"reference_allele\", \"variant_allele\", \"function\", \n",
    "                \"variant\", \"type\"]:\n",
    "    df_test_.pop(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d4007",
   "metadata": {},
   "source": [
    "# get train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9268e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total set training\n",
    "\n",
    "part_dict = defaultdict(list)\n",
    "for key, value in function_dict.items():    \n",
    "    for j in range(fold):\n",
    "        if j == test_partition:\n",
    "            continue\n",
    "        part_dict[key].append(value[j])\n",
    "        \n",
    "# balancing traning set\n",
    "for key in part_dict.keys():\n",
    "    part_dict[key] = pd.concat(part_dict[key], axis=0)\n",
    "\n",
    "# upsampling with 2 * max_len\n",
    "max_len = max([len(x) for x in part_dict.values()])\n",
    "for key in part_dict.keys():\n",
    "    part_dict[key] = part_dict[key].sample(n=max_len*2, replace=True, random_state=seed2)\n",
    "\n",
    "df_train = pd.concat(list(part_dict.values()), axis=0)\n",
    "\n",
    "train_label = df_train[\"function\"].values\n",
    "\n",
    "df_train_ = df_train.copy(deep=True)\n",
    "\n",
    "for col in [\"gene\", \"haplotype_name\", \"chr\", \"variant_start\", \n",
    "                \"reference_allele\", \"variant_allele\", \"function\", \n",
    "                \"variant\", \"type\"]:\n",
    "    df_train.pop(col)\n",
    "    \n",
    "# train_data = df_train.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f633b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_dict = {}\n",
    "normalizer_dict = {}\n",
    "for col in df_train.columns:\n",
    "    if \"_variant\" in col or \"_gained\" in col or \"_lost\" in col:\n",
    "        continue\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean').fit(df_train[col].values.reshape(-1, 1))\n",
    "    imputer_dict[col] = imp\n",
    "    df_train[col] = imp.transform(df_train[col].values.reshape(-1, 1))\n",
    "    \n",
    "    normalizer = StandardScaler().fit(df_train[col].values.reshape(-1, 1))\n",
    "    normalizer_dict[col] = normalizer\n",
    "    df_train[col] = normalizer.transform(df_train[col].values.reshape(-1, 1))\n",
    "\n",
    "with open(\"model/imputer_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(imputer_dict, f)\n",
    "    \n",
    "with open(\"model/normalizer_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(normalizer_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f6a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/imputer_dict.pkl\", \"rb\") as f:\n",
    "    imputer_dict = pickle.load(f)\n",
    "    \n",
    "with open(\"model/normalizer_dict.pkl\", \"rb\") as f:\n",
    "    normalizer_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "551686a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in imputer_dict.keys():\n",
    "    df_test_[key] = imputer_dict[key].transform(df_test_[key].values.reshape(-1, 1))\n",
    "    df_test_[key] = normalizer_dict[key].transform(df_test_[key].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8bfb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补充关键数据项数据\n",
    "df_train_key = df_train.copy(deep=True)\n",
    "col_list = [\n",
    "        'DEOGEN2_score', 'M-CAP_score', 'MPC_score', 'MutationAssessor_score',\n",
    "        'LRT_score', 'FATHMM_score', 'PROVEAN_score',\n",
    "        'Polyphen2_HVAR_score', 'integrated_fitCons_score', 'VEST4_score',\n",
    "        'SIFT4G_score', 'LoFtool', 'GenoCanyon_score', 'CADD_raw', 'APF_score'\n",
    "    ]\n",
    "for col in df_train_key.columns:\n",
    "    if \"_variant\" in col or \"_gained\" in col or \"_lost\" in col:\n",
    "        col_list.append(col)\n",
    "        continue\n",
    "        \n",
    "    if col not in [\n",
    "        'DEOGEN2_score', 'M-CAP_score', 'MPC_score', 'MutationAssessor_score',\n",
    "        'LRT_score', 'FATHMM_score', 'PROVEAN_score',\n",
    "        'Polyphen2_HVAR_score', 'integrated_fitCons_score', 'VEST4_score',\n",
    "        'SIFT4G_score', 'LoFtool', 'GenoCanyon_score', 'CADD_raw', 'APF_score'\n",
    "    ]:\n",
    "        df_train_key[col] = [np.nan] * len(df_train_key)\n",
    "        df_train_key[col] = imputer_dict[col].transform(df_train_key[col].values.reshape(-1, 1))\n",
    "        df_train_key[col] = normalizer_dict[col].transform(df_train_key[col].values.reshape(-1, 1))\n",
    "        \n",
    "        \n",
    "# df_train_all = pd.concat([df_train, df_train_key], axis=0)\n",
    "\n",
    "df_train_key = df_train.copy(deep=True)\n",
    "\n",
    "df_train = df_train[col_list]\n",
    "df_test_ = df_test_[col_list]\n",
    "\n",
    "for col in df_train_key.columns:\n",
    "    if \"_variant\" in col or \"_gained\" in col or \"_lost\" in col:\n",
    "        continue\n",
    "        \n",
    "    if col not in [\n",
    "        'DEOGEN2_score', 'M-CAP_score', 'MPC_score', 'MutationAssessor_score',\n",
    "        'LRT_score', 'FATHMM_score', 'PROVEAN_score',\n",
    "        'Polyphen2_HVAR_score', 'integrated_fitCons_score', 'VEST4_score',\n",
    "        'SIFT4G_score', 'LoFtool', 'GenoCanyon_score', 'CADD_raw', 'APF_score'\n",
    "    ]:\n",
    "        df_train_key[col] = [np.nan] * len(df_train_key)\n",
    "        df_train_key[col] = imputer_dict[col].transform(df_train_key[col].values.reshape(-1, 1))\n",
    "        df_train_key[col] = normalizer_dict[col].transform(df_train_key[col].values.reshape(-1, 1))\n",
    "        \n",
    "        \n",
    "# df_train_all = pd.concat([df_train_all, df_train_key], axis=0)\n",
    "\n",
    "df_train_all = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93836ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(test_label, result, y_pred=None):\n",
    "    result = [\"neutral\" if x == \"neutrual\" else x for x in result]\n",
    "    test_label = [\"neutral\" if x == \"neutrual\" else x for x in test_label]\n",
    "    test_label_ = [0 if x == \"neutral\" else 1 for x in test_label]\n",
    "    \n",
    "    if y_pred is not None:\n",
    "        roc_auc = roc_auc_score(test_label_, y_pred)\n",
    "        print(\"roc_auc:\", round(roc_auc, 4))\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(test_label_, y_pred)\n",
    "        f1 = [(2 * precision[i] * recall[i]) / (precision[i] + recall[i]) for i in range(len(thresholds))]\n",
    "        max_thres = max(zip(f1, thresholds), key=lambda x: x[0])\n",
    "        print(\"best threshold: \", round(max_thres[1], 4))\n",
    "        print(\"best f1: \", round(max_thres[0], 4))\n",
    "        \n",
    "        prc_auc = auc(recall, precision)\n",
    "        print(\"prc_auc:\", round(prc_auc, 4))\n",
    "    \n",
    "    _test_label = [0 if x == \"neutral\" else 1 for x in test_label]\n",
    "    _result = [0 if x == \"neutral\" else 1 for x in result]\n",
    "    tn, fp, fn, tp = confusion_matrix(test_label, result).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    print(\"precision:\", round(precision, 4))\n",
    "    print(\"sensitivity/recall:\", round(recall, 4))\n",
    "    print(\"f1:\", round(f1, 4))\n",
    "    print(\"specificity:\", round(specificity, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "014ecdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(y_true, y_pred, pos_label=None):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=pos_label)\n",
    "    # x: fpr 假阳率\n",
    "    # y: tpr 召回率\n",
    "    roc_auc = round(auc(fpr, tpr), 4)\n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ed0ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_fillna = df_test_.values\n",
    "train_data_fillna = df_train_all.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d3f02",
   "metadata": {},
   "source": [
    "# --------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d5e3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_dict = {}\n",
    "auc_value_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ace4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 0.9267\n",
      "best threshold:  0.1962\n",
      "best f1:  0.9421\n",
      "prc_auc: 0.9742\n",
      "precision: 0.6957\n",
      "sensitivity/recall: 0.8\n",
      "f1: 0.7442\n",
      "specificity: 0.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\drkg\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# xgboost model\n",
    "xgb_model = XGBClassifier(\n",
    "    learning_rate=0.2,\n",
    "    subsample=0.6,\n",
    "    max_depth=4,\n",
    "    n_estimators=80,\n",
    "    objective=\"binary:logistic\"\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    train_data_fillna, list(train_label),\n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# result = list(xgb_model.predict(test_data_fillna))\n",
    "y_pred = xgb_model.predict_proba(test_data_fillna)[:,0]\n",
    "result = [\"neutral\" if x <= 0.5 else \"deleterious\" for x in y_pred]\n",
    "xgb_model.result = result\n",
    "xgb_model.result_proba = y_pred\n",
    "\n",
    "get_performance(test_label, result, y_pred)\n",
    "\n",
    "fpr, tpr, roc_auc = get_auc(test_label, y_pred, pos_label=\"deleterious\")\n",
    "auc_dict[\"xgboost_fpr\"] = fpr\n",
    "auc_dict[\"xgboost_tpr\"] = tpr\n",
    "auc_value_dict[\"xgboost\"] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aff2b894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 0.9058\n",
      "best threshold:  0.1653\n",
      "best f1:  0.9333\n",
      "prc_auc: 0.9616\n",
      "precision: 0.68\n",
      "sensitivity/recall: 0.85\n",
      "f1: 0.7556\n",
      "specificity: 0.8667\n"
     ]
    }
   ],
   "source": [
    "# lightgbm model\n",
    "lgb_model = LGBMClassifier(\n",
    "    max_depth=4, \n",
    "    learning_rate=0.2, \n",
    "    n_estimators=60,\n",
    "    subsample=0.6, \n",
    "    reg_lambda=0.1\n",
    ")\n",
    "\n",
    "_train_label = [0 if x == \"neutral\" else 1 for x in train_label]\n",
    "_test_label = [0 if x == \"neutral\" else 1 for x in test_label]\n",
    "\n",
    "lgb_model.fit(\n",
    "   train_data_fillna, _train_label,\n",
    ")\n",
    "\n",
    "y_pred = lgb_model.predict_proba(test_data_fillna)[:,1]\n",
    "result = [\"neutral\" if x <= 0.5 else \"deleterious\" for x in y_pred]\n",
    "lgb_model.result = result\n",
    "lgb_model.result_proba = y_pred\n",
    "\n",
    "get_performance(test_label, result, y_pred)\n",
    "\n",
    "fpr, tpr, roc_auc = get_auc(test_label, y_pred, pos_label=\"deleterious\")\n",
    "auc_dict[\"lightgbm_fpr\"] = fpr\n",
    "auc_dict[\"lightgbm_tpr\"] = tpr\n",
    "auc_value_dict[\"lightgbm\"] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4685cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fm model\n",
    "# 需要对空值进行处理\n",
    "\n",
    "# fm_model = FMModel(\n",
    "#     task='binary', \n",
    "#     init=0.1,\n",
    "#     epoch=50, \n",
    "#     lr=0.12,\n",
    "#     reg_lambda=0.01,\n",
    "#     stop_window=2,\n",
    "#     k=4,\n",
    "# )\n",
    "\n",
    "# _train_label = [0 if x == \"neutral\" else 1 for x in train_label]\n",
    "# _test_label = [0 if x == \"neutral\" else 1 for x in test_label]\n",
    "\n",
    "# fm_model.fit(\n",
    "#    train_data_fillna, _train_label * 2,\n",
    "#     is_lock_free=False\n",
    "# )\n",
    "\n",
    "# y_pred = fm_model.predict(test_data_fillna)\n",
    "# result = [\"neutral\" if x <= 0.5 else \"deleterious\" for x in y_pred]\n",
    "# fm_model.result = result\n",
    "# fm_model.result_proba = y_pred\n",
    "\n",
    "# get_performance(test_label, result, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e84bdf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr model\n",
    "# 需要对空值进行处理\n",
    "# lr_model = LRModel(\n",
    "#     task='binary', \n",
    "#     init=0.1,\n",
    "#     epoch=50, \n",
    "#     lr=0.01,\n",
    "#     reg_lambda=0.2,\n",
    "#     stop_window=4,\n",
    "# )\n",
    "\n",
    "# _train_label = [0 if x == \"neutral\" else 1 for x in train_label]\n",
    "# _test_label = [0 if x == \"neutral\" else 1 for x in test_label]\n",
    "\n",
    "# lr_model.fit(\n",
    "#    train_data_fillna, _train_label * 2,\n",
    "#     is_lock_free=False\n",
    "# )\n",
    "\n",
    "# y_pred = lr_model.predict(test_data_fillna)\n",
    "# result = [\"neutral\" if x <= 0.5 else \"deleterious\" for x in y_pred]\n",
    "# lr_model.result = result\n",
    "# lr_model.result_proba = y_pred\n",
    "\n",
    "# get_performance(test_label, result, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cad29548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 0.9333\n",
      "best threshold:  0.2413\n",
      "best f1:  0.9355\n",
      "prc_auc: 0.9771\n",
      "precision: 0.7619\n",
      "sensitivity/recall: 0.8\n",
      "f1: 0.7805\n",
      "specificity: 0.9167\n"
     ]
    }
   ],
   "source": [
    "# rf model\n",
    "rf_model = RandomForestClassifier(\n",
    "    max_depth=6,\n",
    "    random_state=206\n",
    ")\n",
    "\n",
    "rf_model.fit(\n",
    "   train_data_fillna, _train_label,\n",
    ")\n",
    "\n",
    "y_pred = rf_model.predict_proba(test_data_fillna)[:,1]\n",
    "result = [\"neutral\" if x <= 0.5 else \"deleterious\" for x in y_pred]\n",
    "rf_model.result = result\n",
    "rf_model.result_proba = y_pred\n",
    "\n",
    "get_performance(test_label, result, y_pred)\n",
    "\n",
    "fpr, tpr, roc_auc = get_auc(test_label, y_pred, pos_label=\"deleterious\")\n",
    "auc_dict[\"rf_fpr\"] = fpr\n",
    "auc_dict[\"rf_tpr\"] = tpr\n",
    "auc_value_dict[\"rf\"] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cf7d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### df_test_show = df_test[[\"gene\", \"haplotype_name\", \"chr\", \"variant_start\", \"reference_allele\", \"variant_allele\", \"function\"]]\n",
    "# df_test_show.index = range(len(df_test_show))\n",
    "# df_test_show[\"function_prediction\"] = result\n",
    "# df_test_show[\"function\"] = test_label\n",
    "# df_test_show = df_test_show.sort_values(by=[\"gene\", \"function\"])\n",
    "# df_test_show.index = range(len(df_test_show))\n",
    "# df_test_show = df_test_show.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "412c1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def most_common(lst):\n",
    "#     return max(lst, key=lst.count)\n",
    "\n",
    "# hard_vote = [\n",
    "#     most_common(x) for x in list(zip(\n",
    "#         lr_model.result,\n",
    "#         fm_model.result,\n",
    "#         xgb_model.result,\n",
    "#         lgb_model.result,\n",
    "#         rf_model.result\n",
    "#     ))\n",
    "# ]\n",
    "\n",
    "# get_performance(test_label, hard_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bb6e462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7273\n",
      "sensitivity/recall: 0.8\n",
      "f1: 0.7619\n",
      "specificity: 0.9\n"
     ]
    }
   ],
   "source": [
    "# xgb, lgb, rf\n",
    "def most_common(lst):\n",
    "    return max(lst, key=lst.count)\n",
    "\n",
    "hard_vote = [\n",
    "    most_common(x) for x in list(zip(\n",
    "        xgb_model.result,\n",
    "        lgb_model.result,\n",
    "        rf_model.result\n",
    "    ))\n",
    "]\n",
    "\n",
    "get_performance(test_label, hard_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10169c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft_vote_proba = [sum(x) for x in\n",
    "#     list(zip(\n",
    "#         lr_model.result_proba,\n",
    "#         fm_model.result_proba,\n",
    "#         xgb_model.result_proba,\n",
    "#         lgb_model.result_proba,\n",
    "#         rf_model.result_proba\n",
    "#     ))\n",
    "# ]\n",
    "\n",
    "# soft_vote = [\"deleterious\" if x > 1.63 else \"neutral\" for x in soft_vote_proba]\n",
    "\n",
    "# get_performance(test_label, soft_vote, soft_vote_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a04e124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 0.935\n",
      "best threshold:  0.7183\n",
      "best f1:  0.9421\n",
      "prc_auc: 0.9778\n",
      "precision: 0.8\n",
      "sensitivity/recall: 0.8\n",
      "f1: 0.8\n",
      "specificity: 0.9333\n"
     ]
    }
   ],
   "source": [
    "# xgb, lgb, rf\n",
    "soft_vote_proba = [sum(x) for x in\n",
    "    list(zip(\n",
    "        xgb_model.result_proba,\n",
    "        lgb_model.result_proba,\n",
    "        rf_model.result_proba\n",
    "    ))\n",
    "]\n",
    "\n",
    "soft_vote = [\"deleterious\" if x > 0.75 else \"neutral\" for x in soft_vote_proba]\n",
    "\n",
    "get_performance(test_label, soft_vote, soft_vote_proba)\n",
    "\n",
    "fpr, tpr, roc_auc = get_auc(test_label, soft_vote_proba, pos_label=\"deleterious\")\n",
    "auc_dict[\"soft_vote_fpr\"] = fpr\n",
    "auc_dict[\"soft_vote_tpr\"] = tpr\n",
    "auc_value_dict[\"soft_vote\"] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c1c4f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 0.8642\n",
      "best threshold:  0.167\n",
      "best f1:  0.9062\n",
      "prc_auc: 0.9454\n",
      "precision: 0.4595\n",
      "sensitivity/recall: 0.85\n",
      "f1: 0.5965\n",
      "specificity: 0.6667\n"
     ]
    }
   ],
   "source": [
    "# APF\n",
    "y_pred = test_apf\n",
    "result = [\"neutral\" if x <= 0.5 else \"deleterious\" for x in y_pred]\n",
    "\n",
    "get_performance(test_label, result, y_pred)\n",
    "\n",
    "fpr, tpr, roc_auc = get_auc(test_label, y_pred, pos_label=\"deleterious\")\n",
    "auc_dict[\"apf_fpr\"] = fpr\n",
    "auc_dict[\"apf_tpr\"] = tpr\n",
    "auc_value_dict[\"apf\"] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b37772d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_len = max([len(x) for x in auc_dict.values()])\n",
    "\n",
    "for key, value in auc_dict.items():\n",
    "    if len(value) < max_len:\n",
    "        auc_dict[key] = list(value) + [np.nan] * (max_len - len(value))\n",
    "        \n",
    "pd.DataFrame(auc_dict).to_excel(\"output/auc.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5278d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all model\n",
    "# xlearn dump file has bug\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "today_str = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "def save_model():\n",
    "    with open(\"model/xgb_binary_{}.pkl\".format(today_str), \"wb\") as f:\n",
    "        pickle.dump(xgb_model, f)\n",
    "    \n",
    "    with open(\"model/lgb_binary_{}.pkl\".format(today_str), \"wb\") as f:\n",
    "        pickle.dump(lgb_model, f)\n",
    "    \n",
    "    with open(\"model/rf_binary_{}.pkl\".format(today_str), \"wb\") as f:\n",
    "        pickle.dump(rf_model, f)\n",
    "        \n",
    "    with open(\"model/col_list_binary_{}.pkl\".format(today_str), \"wb\") as f:\n",
    "        pickle.dump(col_list, f)\n",
    "        \n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237202dc",
   "metadata": {},
   "source": [
    "# test missing feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2e176c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缺少特征时只使用随机森林作为分类器\n",
    "# with open(\"model/imputer_dict.pkl\", \"rb\") as f:\n",
    "#     imputer_dict = pickle.load(f)\n",
    "\n",
    "# with open(\"model/normalizer_dict.pkl\", \"rb\") as f:\n",
    "#     normalizer_dict = pickle.load(f)\n",
    "\n",
    "# with open(\"model/rf_binary_0816.pkl\", \"rb\") as f:\n",
    "#     rf_model = pickle.load(f)\n",
    "    \n",
    "df_test_key = df_test_.copy(deep=True)\n",
    "\n",
    "# # 将非必须列设为空，测试性能\n",
    "# for col in df_test_key.columns:\n",
    "#     if \"_variant\" in col or \"_gained\" in col or \"_lost\" in col:\n",
    "#         continue\n",
    "        \n",
    "#     if col not in [\n",
    "#         \"integrated_fitCons_score\", \"MetaLR_score\", \"MPC_score\",\n",
    "#         \"CADD_raw\", \"SIFT4G_score\", \"BayesDel_addAF_score\", \"LRT_score\",\n",
    "#         \"MutationAssessor_score\", \"PROVEAN_score\", \"VEST4_score\", \"DEOGEN2_score\",\n",
    "#         \"FATHMM_score\", \"LoFtool\", \"APF_score\"\n",
    "#     ]:\n",
    "#         df_test_key[col] = [np.nan] * len(df_test_key)\n",
    "        \n",
    "\n",
    "# df_test_key = df_test_.copy(deep=True)\n",
    "\n",
    "# # 将非必须列设为空，测试性能\n",
    "# for col in df_test_key.columns:\n",
    "#     if \"_variant\" in col or \"_gained\" in col or \"_lost\" in col:\n",
    "#         continue\n",
    "        \n",
    "#     if col not in [\n",
    "#         \"MetaLR_score\", \"CADD_raw\", \"SIFT4G_score\", \"LRT_score\",\n",
    "#         \"MutationAssessor_score\", \"PROVEAN_score\", \"VEST4_score\", \n",
    "#         \"LoFtool\", \"APF_score\"\n",
    "#     ]:\n",
    "#         df_test_key[col] = [np.nan] * len(df_test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fcf0ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in imputer_dict.keys():\n",
    "    df_test_key[key] = imputer_dict[key].transform(df_test_key[key].values.reshape(-1, 1))\n",
    "    df_test_key[key] = normalizer_dict[key].transform(df_test_key[key].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "28c4b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_key[\"variant_start\"] = test_variant_start\n",
    "df_test_key[\"result\"] = test_label\n",
    "df_test_key[\"apf_raw\"] = test_apf\n",
    "\n",
    "df_test_apf = df_test_key[df_test_key[\"variant_start\"].isin(df_apf[\"variant_start\"].values)]\n",
    "apf_test_label = df_test_apf[\"result\"].values\n",
    "apf_test_apf = df_test_apf[\"apf_raw\"].values\n",
    "\n",
    "df_test_key.pop(\"result\")\n",
    "df_test_apf.pop(\"result\")\n",
    "df_test_key.pop(\"variant_start\")\n",
    "df_test_apf.pop(\"variant_start\")\n",
    "df_test_key.pop(\"apf_raw\")\n",
    "df_test_apf.pop(\"apf_raw\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6fe7fce0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our dataset:\n",
      "hard:\n",
      "precision: 0.6154\n",
      "sensitivity/recall: 0.9231\n",
      "f1: 0.7385\n",
      "specificity: 0.7692\n",
      "soft:\n",
      "roc_auc: 0.9107\n",
      "best threshold:  1.0498\n",
      "best f1:  0.896\n",
      "prc_auc: 0.967\n",
      "precision: 0.5455\n",
      "sensitivity/recall: 0.9231\n",
      "f1: 0.6857\n",
      "specificity: 0.6923\n",
      "apf:\n",
      "roc_auc: 0.6278\n",
      "best threshold:  0.143\n",
      "best f1:  0.7143\n",
      "prc_auc: 0.6721\n",
      "precision: 0.5294\n",
      "sensitivity/recall: 0.6\n",
      "f1: 0.5625\n",
      "specificity: 0.5556\n"
     ]
    }
   ],
   "source": [
    "xgb_pred = xgb_model.predict_proba(df_test_key.values)[:, 0]\n",
    "lgb_pred = lgb_model.predict_proba(df_test_key.values)[:, 1]\n",
    "rf_pred = rf_model.predict_proba(df_test_key.values)[:,1]\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(lst, key=lst.count)\n",
    "\n",
    "hard_vote = [\n",
    "    most_common(x) for x in list(zip(\n",
    "        [\"neutral\" if x <= 0.5 else \"deleterious\" for x in xgb_pred],\n",
    "        [\"neutral\" if x <= 0.5 else \"deleterious\" for x in lgb_pred],\n",
    "        [\"neutral\" if x <= 0.5 else \"deleterious\" for x in rf_pred]\n",
    "    ))\n",
    "]\n",
    "print(\"our dataset:\")\n",
    "print(\"hard:\")\n",
    "get_performance(test_label, hard_vote)\n",
    "\n",
    "soft_vote_proba = [sum(x) for x in\n",
    "    list(zip(\n",
    "        xgb_pred,\n",
    "        lgb_pred,\n",
    "        rf_pred\n",
    "    ))\n",
    "]\n",
    "\n",
    "soft_vote = [\"neutral\" if x <= 1.6203 else \"deleterious\" for x in soft_vote_proba]\n",
    "print(\"soft:\")\n",
    "get_performance(test_label, soft_vote, soft_vote_proba)\n",
    "\n",
    "# APF\n",
    "y_pred = apf_test_apf\n",
    "result = [\"neutral\" if x <= 0.333 else \"deleterious\" for x in y_pred]\n",
    "print(\"apf:\")\n",
    "get_performance(apf_test_label, result, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1093dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apf dataset:\n",
      "hard:\n",
      "precision: 0.5909\n",
      "sensitivity/recall: 0.8667\n",
      "f1: 0.7027\n",
      "specificity: 0.5\n",
      "soft:\n",
      "roc_auc: 0.8148\n",
      "best threshold:  0.5909\n",
      "best f1:  0.8372\n",
      "prc_auc: 0.8491\n",
      "precision: 0.6923\n",
      "sensitivity/recall: 0.6\n",
      "f1: 0.6429\n",
      "specificity: 0.7778\n"
     ]
    }
   ],
   "source": [
    "xgb_pred = xgb_model.predict_proba(df_test_apf.values)[:, 0]\n",
    "lgb_pred = lgb_model.predict_proba(df_test_apf.values)[:, 1]\n",
    "rf_pred = rf_model.predict_proba(df_test_apf.values)[:,1]\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(lst, key=lst.count)\n",
    "\n",
    "hard_vote = [\n",
    "    most_common(x) for x in list(zip(\n",
    "        [\"neutral\" if x <= 0.5 else \"deleterious\" for x in xgb_pred],\n",
    "        [\"neutral\" if x <= 0.5 else \"deleterious\" for x in lgb_pred],\n",
    "        [\"neutral\" if x <= 0.5 else \"deleterious\" for x in rf_pred]\n",
    "    ))\n",
    "]\n",
    "print(\"apf dataset:\")\n",
    "print(\"hard:\")\n",
    "get_performance(apf_test_label, hard_vote)\n",
    "\n",
    "soft_vote_proba = [sum(x) for x in\n",
    "    list(zip(\n",
    "        xgb_pred,\n",
    "        lgb_pred,\n",
    "        rf_pred\n",
    "    ))\n",
    "]\n",
    "\n",
    "soft_vote = [\"neutral\" if x <= 0.75 else \"deleterious\" for x in soft_vote_proba]\n",
    "print(\"soft:\")\n",
    "get_performance(apf_test_label, soft_vote, soft_vote_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65de192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205056d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
